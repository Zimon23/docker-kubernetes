Docker Swarm에서 서비스(Service)는 클러스터 내에서 실행되기를 원하는 애플리케이션의 "설계도" 또는
"청사진"입니다. 단순히 컨테이너 하나를 실행하는 docker run 명령어와는 근본적으로 다릅니다. 서비스는
애플리케이션이 어떤 상태를 유지해야 하는지(Desired State)를 정의하고, Swarm 매니저는 그 상태를 유지하기
위해 지속적으로 작동합니다.

만약 서비스에 정의된 컨테이너(Task) 중 하나가 중단되거나 실패하면, Swarm 매니저는 자동으로 새로운
컨테이너를 생성하여 정의된 상태를 복구합니다.

주요 특징 및 개념

 1. 선언적(Declarative) 모델:
     * docker run은 "이 컨테이너를 지금 실행해"라는 명령적(Imperative) 방식입니다.
     * docker service create는 "이 이미지로 컨테이너 3개가 항상 실행되도록 유지해줘"라는
       선언적(Declarative) 방식입니다. 사용자는 원하는 최종 상태를 선언하기만 하면, Swarm이 그 상태를
       만드는 방법과 유지보수를 책임집니다.

 2. 확장성 (Scalability):
     * 하나의 서비스는 여러 개의 동일한 컨테이너(복제본, Replicas)로 구성될 수 있습니다.
     * docker service scale 명령어를 사용하여 트래픽 변화에 따라 애플리케이션의 복제본 수를 동적으로
       늘리거나 줄일 수 있습니다.

 3. 원하는 상태 유지 (Desired State Reconciliation):
     * Swarm 매니저는 서비스의 현재 상태를 지속적으로 모니터링합니다.
     * 만약 노드(서버)가 다운되거나 컨테이너에 오류가 발생하여 복제본 수가 선언된 수보다 적어지면,
       매니저는 즉시 다른 정상 노드에 새로운 컨테이너를 생성하여 원하는 상태를 복구합니다. 이를 통해
       고가용성(High Availability)을 보장합니다.

 4. 내장 로드 밸런싱 (Built-in Load Balancing):
     * 서비스를 생성하고 포트를 외부에 노출하면, Swarm은 Ingress Routing Mesh라는 내부 로드 밸런서를
       자동으로 설정합니다.
     * 클러스터 내의 어떤 노드로 요청을 보내든, Swarm은 해당 요청을 서비스 내에서 실행 중인 건강한
       컨테이너 중 하나로 알아서 전달해줍니다. 사용자는 개별 컨테이너의 IP 주소를 알 필요가 없습니다.

 5. 무중단 롤링 업데이트 (Rolling Updates):
     * 서비스의 이미지 버전 업그레이드나 설정을 변경할 때, 모든 컨테이너를 한 번에 중단시키는 대신
       순차적으로 하나씩 업데이트를 진행할 수 있습니다.
     * 이를 통해 사용자는 서비스 중단 없이 안전하게 애플리케이션을 배포하고 업데이트할 수 있습니다.
       업데이트 간격, 병렬 처리 수 등을 세밀하게 제어할 수 있습니다.


서비스 예제 (Nginx 웹서버 배포)

아래 예제를 통해 간단한 Nginx 웹서버를 서비스로 배포하고 관리하는 과정을 보여드리겠습니다.

(사전 준비) Docker Swarm 클러스터가 구성되어 있어야 합니다.
만약 클러스터가 없다면, 아래 명령어로 간단히 테스트용 단일 노드 클러스터를 만들 수 있습니다.
 1 docker swarm init

1. 서비스 생성

3개의 복제본(Replicas)을 가진 my-webserver라는 이름의 Nginx 서비스를 생성합니다. 외부의 8080 포트로
들어오는 요청을 컨테이너 내부의 80 포트로 전달합니다.

 docker service create --name my-web --publish published=8080,target=80 --replicas 3 nginx:latest

 * --name my-webserver: 서비스의 고유한 이름을 지정합니다.
 * --publish published=8080,target=80: 호스트(Swarm 클러스터)의 8080 포트를 컨테이너의 80 포트에 매핑합니다.
 * --replicas 3: 이 서비스를 위해 3개의 동일한 컨테이너를 실행하도록 지시합니다.
 * nginx:latest: 서비스가 사용할 도커 이미지입니다.

2. 서비스 상태 확인

현재 Swarm에서 실행 중인 모든 서비스 목록을 확인합니다.

 docker service ls
 실행결과 :

 1 ID             NAME           MODE         REPLICAS   IMAGE          PORTS
 2 u29tq453g1r9   my-web   replicated   3/3        nginx:latest   *:8080->80/tcp
 
REPLICAS 항목이 3/3으로 표시되면 3개의 컨테이너가 모두 정상적으로 실행 중이라는 의미입니다.

3. 서비스의 상세 정보 (컨테이너 목록) 확인

my-web 서비스를 구성하는 개별 컨테이너(Task)들의 상태와 어느 노드에서 실행 중인지 확인합니다.

 docker service ps my-web
출력 예시:

ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE            ERROR     PORTS
cyi2tweseqrp   my-web.1   nginx:latest   manager   Running         Running 30 seconds ago
1e5qs5v3nc5m   my-web.2   nginx:latest   node1     Running         Running 29 seconds ago
04pt62z24qt2   my-web.3   nginx:latest   node2     Running         Running 28 seconds ago


4. 서비스 접속 테스트

웹 브라우저나 curl을 이용해 Swarm 클러스터의 아무 노드 IP 또는 localhost의 8080 포트로 접속하여 Nginx
기본 페이지가 보이는지 확인합니다.

curl localhost:8080
"Welcome to nginx!" 메시지가 보이면 성공입니다.

5. 서비스 확장 (Scaling)

트래픽 증가에 대비해 컨테이너 수를 3개에서 5개로 늘립니다.

docker service scale my-web=5

다시 docker service ls나 docker service ps my-webserver로 확인해보면 복제본 수가 5개로 늘어난 것을 볼 수
있습니다. Swarm 매니저가 자동으로 2개의 컨테이너를 추가로 생성합니다.

docker service ps my-web
출력 예시:

ID             NAME       IMAGE          NODE      DESIRED STATE   CURRENT STATE                ERROR     PORTS
cyi2tweseqrp   my-web.1   nginx:latest   manager   Running         Running about a minute ago
1e5qs5v3nc5m   my-web.2   nginx:latest   node1     Running         Running about a minute ago
04pt62z24qt2   my-web.3   nginx:latest   node2     Running         Running about a minute ago
ii3oq51jzw0x   my-web.4   nginx:latest   node2     Running         Running 8 seconds ago
fe3xha3zk6nz   my-web.5   nginx:latest   manager   Running         Running 9 seconds ago


6. 서비스 롤링 업데이트

실행 중인 Nginx 버전을 httpd (Apache) 이미지로 변경하여 무중단 업데이트를 진행합니다. 한 번에 하나씩,
10초 간격으로 업데이트하도록 설정합니다.

docker service update --image httpd:latest --update-delay 10s my-web
출력 예시:
my-web
overall progress: 5 out of 5 tasks
1/5: running   [==================================================>]
2/5: running   [==================================================>]
3/5: running   [==================================================>]
4/5: running   [==================================================>]
5/5: running   [==================================================>]
verify: Service my-web converged

docker service ps my-web 명령어를 반복해서 실행해보면, 기존 nginx 컨테이너들이 순차적으로 종료되고
새로운 httpd 컨테이너들이 생성되는 과정을 실시간으로 확인할 수 있습니다.

7. 서비스 제거

더 이상 필요 없는 서비스를 제거합니다. 이 명령어를 실행하면 Swarm은 해당 서비스의 정의를 삭제하고 관련된
모든 컨테이너를 중지 및 제거합니다.

docker service rm my-web 

8. Docker Swarm 내장 로드 밸런싱 (Ingress Routing Mesh) 예제 

Docker Swarm의 내장 로드 밸런싱 기능인 Ingress Routing Mesh가 어떻게 동작하는지 보여주는 예제입니다.

핵심은 "어떤 노드로 접속하든, Swarm이 알아서 서비스의 컨테이너로 요청을 분배한다"는 것입니다.

이것을 확인하기 위해, 자신의 컨테이너 ID(호스트명)를 보여주는 간단한 웹 서비스를 띄우고, 여러 번
접속했을 때 응답하는 컨테이너가 계속 바뀌는 것을 관찰해 보겠습니다.

8.1 테스트용 FastAPI 애플리케이션 파일 (main.py)

mkdir load-balancer && cd load-balancer 

vi main.py
----------------------------------------------------------
# main.py
from fastapi import FastAPI, Request
import os
import socket

# FastAPI 애플리케이션 인스턴스를 생성합니다.
app = FastAPI(title="FastAPI Whoami")

@app.get("/")
 async def read_root(request: Request):
     """
     호스트 및 수신 요청에 대한 정보를 반환합니다.
     """
     
     # 현재 호스트의 이름을 가져옵니다.
     hostname = socket.gethostname()
     
     # Docker 컨테이너는 HOSTNAME 환경 변수에 컨테이너 ID를 설정합니다.
     # 이 값을 우선 사용하고, 없으면 일반 호스트 이름을 사용합니다.
     container_id = os.getenv("HOSTNAME", hostname)

     # 요청 정보를 포함하는 딕셔너리를 반환합니다.
     return {
         "hostname"    : container_id,                 # 컨테이너의 호스트명 (ID)
         "ip_address"  : request.client.host,          # 요청을 보낸 클라이언트의 IP 주소
         "headers"     : dict(request.headers),        # 모든 HTTP 헤더
         "method"      : request.method,               # HTTP 요청 메서드 (GET, POST 등)
         "path"        : request.url.path,             # 요청 경로
         "query_params": dict(request.query_params),   # 쿼리 파라미터
         "message"     : f"Hello from {container_id}!" # 환영 메시지
     }

 @app.get("/health")
 async def health_check():
     """
     간단한 헬스 체크 엔드포인트입니다.
     """
     return {"status": "ok"}
---------------------------------------------------------

  8.2 의존성 파일 (requirements.txt)
---------------------------------------------------------
# requirements.txt
# FastAPI 웹 프레임워크
fastapi
# ASGI 서버 구현체 (FastAPI 애플리케이션을 실행하는 데 사용)
uvicorn
---------------------------------------------------------

  8.3 Dockerfile 
---------------------------------------------------------
# Dockerfile
# Python 3.10 slim-buster 이미지를 기본 이미지로 사용합니다.
FROM python:3.10-slim-buster

# 컨테이너 내부의 작업 디렉토리를 /app으로 설정합니다.
WORKDIR /app

# 현재 디렉토리의 requirements.txt 파일을 컨테이너의 /app 디렉토리로 복사합니다.
COPY requirements.txt .
# 복사된 requirements.txt에 명시된 Python 의존성 패키지들을 설치합니다.
# --no-cache-dir 옵션은 캐시 디렉토리를 사용하지 않아 이미지 크기를 줄입니다.
RUN pip install --no-cache-dir -r requirements.txt

# 현재 디렉토리의 main.py 파일을 컨테이너의 /app 디렉토리로 복사합니다.
COPY main.py .

# 컨테이너가 8000번 포트를 외부에 노출할 것임을 선언합니다.
# 이는 문서화 목적이며, 실제 포트 매핑은 'docker run -p' 또는 'docker service create --publish'에서 이루어집니다.
EXPOSE 8000

# Uvicorn을 사용하여 FastAPI 애플리케이션을 실행합니다.
# "main:app"은 main.py 파일의 app 객체를 의미합니다.
# --host 0.0.0.0은 모든 네트워크 인터페이스에서 접근 가능하게 합니다.
# --port 8000은 8000번 포트로 서비스를 제공합니다.
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

--------------------------------------------------------

  8.4 사용 방법

   1. 파일 생성:
      위의 내용을 각각 main.py, requirements.txt, Dockerfile이라는 이름으로 동일한 디렉토리에 저장합니다.

   2. Docker 이미지 빌드:
      터미널에서 해당 디렉토리로 이동하여 다음 명령어를 실행하여 Docker 이미지를 빌드합니다.
      (여기서는 my-fastapi-whoami라는 태그를 사용합니다.)

      docker build -t my-fastapi-whoami .

   3. Docker Swarm 서비스 생성:
      이제 이 이미지를 사용하여 Docker Swarm 서비스를 생성합니다. traefik/whoami 예제와 동일하게 3개의 복제본을 만들고, 클러스터의 8000번 포트를 컨테이너의 8000번 포트에 매핑합니다.

      docker service create \
        --name my-fastapi-whoami-app \
        --publish published=8000,target=8000 \
        --replicas 3 \
        my-fastapi-whoami
      주의: `--publish`의 `target` 포트는 Dockerfile에서 `EXPOSE`한 포트와 `CMD`에서 Uvicorn이 바인딩하는 포트(8000)와 일치해야 합니다.

   4. 서비스 상태 확인:
      서비스가 정상적으로 실행 중인지 확인합니다.

      docker service ps my-fastapi-whoami-app

   5. 로드 밸런싱 테스트:
      traefik/whoami 예제와 동일하게 여러 번 요청을 보내면서 hostname 값이 바뀌는 것을 확인합니다.

      vi run.sh 
      
      #/bin/sh
      for i in {1..6}; do curl -s localhost:8000 | grep hostname; sleep 1; done

      chmod +x run.sh 
      
      ./run.sh 
      
      출력 예시:

       1 Hostname: 5b8e3a3c8e9c
       2 Hostname: 9e4a1d2b7f6a
       3 Hostname: 3c7d6f5e2a1b
       4 Hostname: 5b8e3a3c8e9c
       5 Hostname: 3c7d6f5e2a1b
       6 Hostname: 9e4a1d2b7f6a

      결과 분석

      위 출력 결과를 보면, 단일 주소(`localhost:8000`)로 요청을 보냈음에도 불구하고 응답하는 `Hostname`이 계속
      바뀌는 것을 볼 수 있습니다.

       * 첫 번째 요청은 5b8e... 컨테이너가 응답했습니다.
       * 두 번째 요청은 9e4a... 컨테이너가 응답했습니다.
       * 세 번째 요청은 3c7d... 컨테이너가 응답했습니다.
       * 이후 요청들도 3개의 컨테이너에 라운드 로빈(Round-Robin) 방식으로 분배됩니다.

      이것이 바로 Ingress Routing Mesh의 역할입니다. 우리는 클러스터의 진입점(여기서는 localhost:8000)에만
      요청을 보냈을 뿐인데, Swarm이 내부적으로 요청을 받아 실행 중인 3개의 컨테이너 중 하나로 알아서 전달하고
      부하를 분산시켜 준 것입니다.

      이 덕분에 사용자는 어떤 노드에 어떤 컨테이너가 실행 중인지 전혀 신경 쓸 필요 없이, 서비스 자체에만
      집중할 수 있습니다.

   6. 서비스 제거
     테스트가 끝났으면 서비스를 제거하여 자원을 정리합니다.
     docker service rm my-fastapi-whoami-app
 
 
